{"cells":[{"cell_type":"markdown","metadata":{"id":"HcXOL15t-2Kf"},"source":["# Presprocessing the Amazon Reviews Dataset"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bMGqvF94OCXL","executionInfo":{"status":"ok","timestamp":1701155099720,"user_tz":480,"elapsed":1277,"user":{"displayName":"Austin Chen","userId":"06999593592342824043"}},"outputId":"9a97a45e-35c6-4151-a6f3-681ee928cb81"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"bfVMxT8k-2Ki","executionInfo":{"status":"ok","timestamp":1701155101874,"user_tz":480,"elapsed":2158,"user":{"displayName":"Austin Chen","userId":"06999593592342824043"}}},"outputs":[],"source":["import gzip\n","import json\n","import os\n","import string\n","import nltk"]},{"cell_type":"code","source":["nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","# Remove punctuation from the text\n","def remove_punctuation(text):\n","  punctuationfree = \"\".join([i for i in text if i not in string.punctuation])\n","  return punctuationfree\n","\n","# Remove stopwords\n","# Slice until 63, keep all negative modifiers\n","stopwords = nltk.corpus.stopwords.words('english')[:63]\n","\n","def remove_stopwords(text):\n","  output = [i for i in text if i not in stopwords]\n","  return output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ivq3Np0PsRTd","outputId":"a0f47cbd-6629-484c-9300-bd5ea9b07926","executionInfo":{"status":"ok","timestamp":1701155102084,"user_tz":480,"elapsed":213,"user":{"displayName":"Austin Chen","userId":"06999593592342824043"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}]},{"cell_type":"code","source":["def process_text(rating, text):\n","  if rating < 3:\n","    sentiment = 0\n","  elif rating < 4:\n","    sentiment = 1\n","  else:\n","    sentiment = 2\n","\n","  text = remove_punctuation(text).lower().replace('\\n', ' ')\n","  tokens = nltk.word_tokenize(text)\n","  tokens = remove_stopwords(tokens)\n","\n","  wordnet_lemmatizer = nltk.stem.WordNetLemmatizer()\n","  lemmatized_text = [wordnet_lemmatizer.lemmatize(word) for word in tokens]\n","\n","  return (lemmatized_text, sentiment)\n"],"metadata":{"id":"orQcMjrnh36T","executionInfo":{"status":"ok","timestamp":1701155102084,"user_tz":480,"elapsed":5,"user":{"displayName":"Austin Chen","userId":"06999593592342824043"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["Now we import the data from Google Drive."],"metadata":{"id":"2zPaUjlSOYIN"}},{"cell_type":"code","source":["folder = '/content/drive/MyDrive/CS 171/Final Project'\n","filename = 'Office_Products_5.json.gz'\n","cleaned_filename = 'Office_Products_Clean.json'"],"metadata":{"id":"MBMJ02bXOaJy","executionInfo":{"status":"ok","timestamp":1701155102274,"user_tz":480,"elapsed":195,"user":{"displayName":"Austin Chen","userId":"06999593592342824043"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["With all utility functions defined, we can now preprocess the data."],"metadata":{"id":"BQydaqHezDHC"}},{"cell_type":"code","execution_count":6,"metadata":{"id":"6XsAGn5b-2Kk","executionInfo":{"status":"ok","timestamp":1701155548633,"user_tz":480,"elapsed":446361,"user":{"displayName":"Austin Chen","userId":"06999593592342824043"}}},"outputs":[],"source":["dataset = []\n","\n","with gzip.open(os.path.join(folder, filename), 'r') as infile:\n","    for line in infile.readlines():\n","      review = json.loads(line)\n","\n","      try:\n","        text, sentiment = process_text(review['overall'], review['reviewText'])\n","      except KeyError:\n","        # Skip reviews with no review text\n","        pass\n","\n","      if len(text) != 0:\n","        dataset.append((text, sentiment))"]},{"cell_type":"markdown","source":["Now, we re-export the data as another json."],"metadata":{"id":"8elPOUfjzOh4"}},{"cell_type":"code","execution_count":7,"metadata":{"id":"oTGjxpqL-2Kl","executionInfo":{"status":"ok","timestamp":1701155582498,"user_tz":480,"elapsed":33879,"user":{"displayName":"Austin Chen","userId":"06999593592342824043"}}},"outputs":[],"source":["with open(os.path.join(folder, cleaned_filename), 'w') as outfile:\n","  for text, sentiment in dataset:\n","    entry_dict = {'text': text, 'sentiment': sentiment}\n","    json.dump(entry_dict, outfile)\n","    outfile.write('\\n')"]},{"cell_type":"code","source":[],"metadata":{"id":"dXJHB1VrujFT","executionInfo":{"status":"ok","timestamp":1701155582498,"user_tz":480,"elapsed":14,"user":{"displayName":"Austin Chen","userId":"06999593592342824043"}}},"execution_count":7,"outputs":[]}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"},"orig_nbformat":4,"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}